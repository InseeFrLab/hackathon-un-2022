{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b21118b-69af-44a0-8917-3cdb17521230",
   "metadata": {},
   "source": [
    "# Choose a Kernel\n",
    "- Please wait 4-5 minutes for the kernel to initialize properly\n",
    "- Keep checking kernel status at the bottom (changes from Initializing to Idle state)\n",
    "- Rename notebook and start coding ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822ad908-33ac-4d0d-b962-e08fe1e1200c",
   "metadata": {},
   "source": [
    "# Reading AIS Data\n",
    "- write your own code to access AIS data (might show you in Workshop) or\n",
    "- Import AIS package from GitLab (recommended) \n",
    "    - get_ais()\n",
    "    - access GitLab using a username and token (shown below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "987e6c85-afa3-4970-a56d-5c064bc6ff80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/sparkuser/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\n",
      "Collecting pyarrow==10.0.0\n",
      "  Downloading pyarrow-10.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 35.3 MB 26.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /opt/conda/lib/python3.8/site-packages (from pyarrow==10.0.0) (1.20.1)\n",
      "Installing collected packages: pyarrow\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 3.0.0\n",
      "    Uninstalling pyarrow-3.0.0:\n",
      "      Successfully uninstalled pyarrow-3.0.0\n",
      "Successfully installed pyarrow-10.0.0\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow==10.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9f376b0-9fa0-43c3-8207-406b69441044",
   "metadata": {},
   "outputs": [],
   "source": [
    "#allow multiple outputs in one jupyter cell\n",
    "from IPython.core.interactiveshell import InteractiveShell \n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "# to apply aggregation functions on spark df\n",
    "import pyspark.sql.functions as F\n",
    "from pyarrow import fs\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d959cca8-a285-42fd-8d79-112098db89d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://read_aistt:****@code.officialstatistics.org/trade-task-team-phase-1/ais.git\n",
      "  Cloning https://read_aistt:****@code.officialstatistics.org/trade-task-team-phase-1/ais.git to /tmp/pip-req-build-6dcw_nwr\n",
      "Building wheels for collected packages: ais\n",
      "  Building wheel for ais (setup.py): started\n",
      "  Building wheel for ais (setup.py): finished with status 'done'\n",
      "  Created wheel for ais: filename=ais-2.7.6-py3-none-any.whl size=9267 sha256=6824bd671d4f0bd36048929ca1b4d3efc2a70c4c7b3ee008313600ff45aa96fc\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-mg0up6se/wheels/49/e0/a2/25d96a62cf626776ab2fd57fcbd822c2b8118049a84b16953d\n",
      "Successfully built ais\n",
      "Installing collected packages: ais\n",
      "Successfully installed ais-2.7.6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this cell contains the code to access GitLab repo\n",
    "# need it to install ais package from GitLab repo\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "GITLAB_USER = \"read_aistt\"  # read only access\n",
    "GITLAB_TOKEN = \"MMQ6ky1rnLsuKxjyZuvB\"\n",
    "\n",
    "# clone the repo and install the ais packag\n",
    "git_package = f\"git+https://{GITLAB_USER}:{GITLAB_TOKEN}@code.officialstatistics.org/trade-task-team-phase-1/ais.git\"\n",
    "\n",
    "std_out = subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", git_package], capture_output=True, text=True).stdout\n",
    "print(std_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2aeda2e-a093-405b-ad10-40726776d1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "AWS_ACCESS_KEY_ID ····················\n",
      "AWS_SECRET_ACCESS_KEY ········································\n",
      "AWS_SESSION_TOKEN ···························································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································································\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "\n",
    "AWS_ACCESS_KEY_ID=getpass.getpass(\"AWS_ACCESS_KEY_ID\")\n",
    "AWS_SECRET_ACCESS_KEY=getpass.getpass(\"AWS_SECRET_ACCESS_KEY\")\n",
    "AWS_SESSION_TOKEN=getpass.getpass(\"AWS_SESSION_TOKEN\")\n",
    "AWS_S3_ENDPOINT=\"minio.lab.sspcloud.fr\"\n",
    "\n",
    "s3 = fs.S3FileSystem(endpoint_override=AWS_S3_ENDPOINT,\n",
    "                     access_key=AWS_ACCESS_KEY_ID, \n",
    "                     secret_key=AWS_SECRET_ACCESS_KEY, \n",
    "                     session_token=AWS_SESSION_TOKEN)\n",
    "\n",
    "BUCKET_OUT = \"projet-hackathon-un-2022\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91762ff2-4f2f-41ab-bfc4-276cd465b6d7",
   "metadata": {},
   "source": [
    "# Using get_ais()\n",
    "- retrieve data for a single date\n",
    "- filter data on date and specific columns\n",
    "- filter data for a range of dates\n",
    "- filter data based on mmsi (unique ship identifier)\n",
    "- fiter data based on a geographical polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c739f75-5735-495b-81ef-71d4a4fe2054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import get_ais() from ais package\n",
    "from ais import functions as af"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc48a59d-5bbd-4b48-a274-af8f91dba182",
   "metadata": {},
   "source": [
    "# 1 Retrive data based on geolocation polygon\n",
    "\n",
    "Here we use different geolocaltion to retrive data and store to sspcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3757d14-f061-4962-9eda-214e1cb21231",
   "metadata": {},
   "source": [
    "## 1.1 Get data of black sea and azov area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d01ab34-4a26-4edc-8dcc-87bcb3a711a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/opt/spark/work-dir/launch_ipykernel.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# pass polygon_hex_df to get_ais()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m df_ais_polygon = af.get_ais(spark,\n\u001b[0m\u001b[1;32m     25\u001b[0m                             \u001b[0mstart_date\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                             \u001b[0mend_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mend_date\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/ais/_aisfilter.py\u001b[0m in \u001b[0;36mget_ais\u001b[0;34m(spark, start_date, end_date, h3_list, polygon_hex_df, mmsi_list, message_type, columns, polygon, polygon_hex_resolution)\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0mpoly_copy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolygon_hex_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'hex_id'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcolumn_filter\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         filtered_ais_df = apply_small_filter(spark,\n\u001b[0m\u001b[1;32m    246\u001b[0m             \u001b[0mfiltered_ais_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0mpoly_copy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/ais/_aisfilter.py\u001b[0m in \u001b[0;36mapply_small_filter\u001b[0;34m(spark, ais_df, filter_criteria, column_filter)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter_criteria\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcolumn_filter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilter_criteria\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mdf_filter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter_criteria\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"filter criteria dataframe should have column_filter\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhas_pandas\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;31m# Create a DataFrame from pandas DataFrame.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m             return super(SparkSession, self).createDataFrame(\n\u001b[0m\u001b[1;32m    604\u001b[0m                 data, schema, samplingRatio, verifySchema)\n\u001b[1;32m    605\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    298\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_from_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimezone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_convert_from_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimezone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    628\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplySchemaToPythonRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchemaFromList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n\u001b[1;32m    381\u001b[0m             warnings.warn(\"inferring schema from dict is deprecated,\"\n\u001b[1;32m    382\u001b[0m                           \"please use pyspark.sql.Row instead\")\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_merge_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_infer_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_has_nulltype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Some of types cannot be determined after inferring\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_merge_type\u001b[0;34m(a, b, name)\u001b[0m\n\u001b[1;32m   1105\u001b[0m                                                   name=new_name(f.name)))\n\u001b[1;32m   1106\u001b[0m                   for f in a.fields]\n\u001b[0;32m-> 1107\u001b[0;31m         \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1108\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnfs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set coordinates of the selected polygons in geojson format\n",
    "# https://boundingbox.klokantech.com/\n",
    "AREA = \"azov_black\"\n",
    "\n",
    "if AREA == \"azov\":\n",
    "    bb = [[32.4143284746,45.0048840974],[40.0827855058,45.0048840974],[40.0827855058,47.9395951189],[32.4143284746,47.9395951189],[32.4143284746,45.0048840974]]\n",
    "elif AREA == \"azov_black\":\n",
    "    bb = [[43.3308500839,39.9913666442],[26.1506878922,41.33737686],[27.1872912828,48.4341912681],[44.3674534746,47.2431326615],[43.3308500839,39.9913666442]]\n",
    "\n",
    "polygon = {\n",
    "        \"type\": \"Polygon\",\n",
    "        \"coordinates\": [bb]\n",
    "    }\n",
    "\n",
    "polygon_hex_df = af.polygon_to_hex_df([(\"Polygon\", polygon)])\n",
    "\n",
    "# Filter boats that were at least 1 time in our polygon\n",
    "\n",
    "start_date = datetime.fromisoformat(\"2022-04-01\")\n",
    "end_date = datetime.fromisoformat(\"2022-04-08\")\n",
    "columns = [\"mmsi\", \"latitude\", \"longitude\", \"eeid\", \"dt_insert_utc\", \"destination\"]\n",
    "\n",
    "# pass polygon_hex_df to get_ais()\n",
    "df_ais_polygon = af.get_ais(spark,\n",
    "                            start_date, \n",
    "                            end_date = end_date,\n",
    "                            columns = columns,\n",
    "                            polygon_hex_df = polygon_hex_df\n",
    "                           )\n",
    "\n",
    "ais_polygon.count()\n",
    "ais_polygon.show(n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430197e2-0266-449d-b399-a8ba1022666d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e022b9-5685-4a26-b149-1e4ae4e6ee71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get full traces of boats to get areas of origin and destination\n",
    "\n",
    "# Get list of boats in our polygon\n",
    "unique_mmsi_polygon = df_ais_polygon.select(F.col(\"mmsi\")).distinct().toPandas()[\"mmsi\"].tolist()\n",
    "\n",
    "# Buffers to ensure getting origin and destination\n",
    "start_date_buffer = datetime.fromisoformat(\"2022-03-25\")\n",
    "end_date_buffer = datetime.fromisoformat(\"2022-04-14\")\n",
    "\n",
    "# Get full traces of all boats that were at least once in our polygon\n",
    "df_full_traces = af.get_ais(spark,\n",
    "start_date_buffer,\n",
    "end_date = end_date_buffer,\n",
    "columns = columns,\n",
    "mmsi_list = unique_mmsi_polygon\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad5c20d-3ea4-400f-9bb4-bf37b690aa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data to S3\n",
    "start_date_str = start_date.strftime(\"%Y%m%d\")\n",
    "end_date_str = end_date.strftime(\"%Y%m%d\")\n",
    "\n",
    "table = pa.Table.from_pandas(df_full_traces.toPandas())\n",
    "\n",
    "pq.write_table(table, f\"projet-hackathon-un-2022/AIS/ais_{AREA}_{start_date_str}_{end_date_str}_full_traces.parquet\", \n",
    "               filesystem=s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f2ce30-b636-4211-9160-4b8be047d0f1",
   "metadata": {},
   "source": [
    "## 1.2 Retrive all Ais data of 01/04 - 08/04  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fd29404-9512-4f9d-9a48-d004c0f71190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198771490"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_date = datetime.fromisoformat(\"2022-04-01\")\n",
    "end_date = datetime.fromisoformat(\"2022-04-08\")\n",
    "columns = [\"mmsi\", \"latitude\", \"longitude\", \"eeid\", \"dt_insert_utc\", \"destination\"]\n",
    "\n",
    "# pass polygon_hex_df to get_ais()\n",
    "df_full_ais_2022_04 = af.get_ais(spark,\n",
    "                            start_date, \n",
    "                            end_date = end_date,\n",
    "                            columns = columns,\n",
    "                           )\n",
    "\n",
    "df_full_ais_2022_04.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3963085c-3203-48ac-b586-7840151a9e8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79d62901-5e39-450e-8f35-8fb3c949cba4",
   "metadata": {},
   "source": [
    "# Accessing IHS Data \n",
    "- ship registry data in s3\n",
    "- includes details about ship on a very granular level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e7fb68-d5cf-4ebc-a340-47b153f70ced",
   "metadata": {},
   "source": [
    "## Ship Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bbf004cf-8018-4b77-8eba-6f5d1ef48f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath = \"s3a://ungp-ais-data-historical-backup/register/\"\n",
    "\n",
    "# first file \n",
    "df_ship_data = spark.read.load(basepath+ \"ShipData.CSV\", \n",
    "                               format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9de59143-5127-4097-81e2-2fd158618c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ship_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "12854537-9ed2-4ab7-a5dc-53835aa2bba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pa.Table.from_pandas(df_ship_data.toPandas())\n",
    "pq.write_table(table, f\"{BUCKET_OUT}/IHS/ship_data.parquet\", filesystem=s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e24ee91-0a79-4005-8494-211366db1516",
   "metadata": {},
   "source": [
    "## Ship Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83ff1bd1-14f6-4e2a-bbf9-d113ba1cca71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second file read ship codes\n",
    "df_ship_code = spark.read.load(basepath + \"tblShipTypeCodes.CSV\", \n",
    "                     format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c698a64-f72f-46b8-843f-ee1bbfaff449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ship_code.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "80b312bf-968f-4b46-936d-0ee5b4f5ecc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pa.Table.from_pandas(df_ship_code.toPandas())\n",
    "pq.write_table(table, f\"{BUCKET_OUT}/IHS/ship_codes.parquet\", filesystem=s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f66697-ce7c-49c6-9b98-5342cf290ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52d8a9a-fe65-42b8-a2b6-948e6bc30bfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cfac0bb5-6878-4671-b9ef-9b13f5a23c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f267f50b-d0d0-49d8-ac34-2bf0a99145a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Config template datadive",
   "language": "python3",
   "name": "datadive"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
